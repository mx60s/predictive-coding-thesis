- The autoencoder they compare their model against is just encoding a single observation, while their model works on an entire sequence.
- Can you build one that takes an sequence as an input, or does that require the self-attention?

- https://github.com/julianstastny/VAE-ResNet18-PyTorch/tree/master no transposed convolutions 